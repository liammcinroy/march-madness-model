# train_models.py
# This uses the data as generated by featuregen.py to train different models
# Developed by Liam McInroy on 11.30.18

import argparse
import json

import numpy as np

from pomegranate import BayesClassifier, MultivariateGaussianDistribution
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold


def train_naive_non_stat_bayes(X, y, **kwargs):
    """Train a naive bayesian model on the given features data, which doesn't
    use many of the team statistics since they're unreliably provided.

    Arguments:
        X: The features generated by feature_gen.py to train from
        y: The classes
        kwargs: For verbosity
            verbose: If greater than zero, then outputs basic information
                about the training process during training. Default 0.
            n_splits: The number of folds to use during KFold cross validation
                Default 5.
    """
    def printverbose(*msg):
        if kwargs.get('verbose', 0) > 0:
            print(*msg)

    # exclude the time series identifying features since we aren't using a
    # temporal model
    _X = X[:, 1:]

    # get the features which don't have unknown values
    cols = [j for j in range(_X.shape[1]) if None not in _X[:, j]]
    printverbose('Valid feature columns:', cols)

    # restrict to not have unknown features
    _X = _X[:, cols]

    # get the rows which have valid data throughout
    rows = [i for i in range(_X.shape[0]) if None not in _X[i]]
    printverbose('Number of valid training samples', len(rows))

    # restrict down
    _X = _X[rows]
    _y = y[rows]

    printverbose('Training on {} samples'.format(_X.shape[0]))

    # begin the training routine. We use K-fold to estimate the accuracy and
    # generalization power of the models
    sk_fold = KFold(n_splits=kwargs.get('n_splits', 5))
    cum_acc = 0
    for k, (train_idx, test_idx) in enumerate(sk_fold.split(_X, _y)):
        clf = BayesClassifier.from_samples(MultivariateGaussianDistribution,
                                           _X[train_idx],
                                           _y[train_idx].flatten())

        acc = accuracy_score(_y[test_idx].flatten(),
                             clf.predict(_X[test_idx]))
        printverbose('Fold {} accuracy: {}'.format(k + 1, acc))
        cum_acc = (k * cum_acc + acc) / (k + 1.)
        printverbose('\tCurrent cumulative accuracy:', cum_acc)

    print('Cumulative accuracy after {} folds: {}'.format(k, cum_acc))


def train_naive_stat_bayes(X, y, **kwargs):
    """Train a naive bayesian classifier on the given features data, including
    the season average statistics for each team.

    Arguments:
        X: The features generated by feature_gen.py to train from
        y: The classes
        kwargs: For verbosity
            verbose: If greater than zero, then outputs basic information
                about the training process during training. Default 0.
            n_splits: The number of folds to use during KFold cross validation
                Default 5.
    """
    def printverbose(*msg):
        if kwargs.get('verbose', 0) > 0:
            print(*msg)

    # exclude the time series identifying features since we aren't using a
    # temporal model
    _X = X[:, 1:]

    # get the rows which have valid data throughout
    rows = [i for i in range(_X.shape[0]) if None not in _X[i]]
    printverbose('Number of valid training samples', len(rows))

    # restrict down
    _X = _X[rows]
    _y = y[rows]

    printverbose('Training on {} samples'.format(_X.shape[0]))

    # begin the training routine. We use K-fold to estimate the accuracy and
    # generalization power of the models
    sk_fold = KFold(n_splits=kwargs.get('n_splits', 5))
    cum_acc = 0
    for k, (train_idx, test_idx) in enumerate(sk_fold.split(_X, _y)):
        clf = BayesClassifier.from_samples(MultivariateGaussianDistribution,
                                           _X[train_idx],
                                           _y[train_idx].flatten())

        acc = accuracy_score(_y[test_idx].flatten(),
                             clf.predict(_X[test_idx]))
        printverbose('Fold {} accuracy: {}'.format(k + 1, acc))
        cum_acc = (k * cum_acc + acc) / (k + 1.)
        printverbose('\tCurrent cumulative accuracy:', cum_acc)

    print('Cumulative accuracy after {} folds: {}'.format(k, cum_acc))


def train_temporal_bayes(data):
    """Train a bayesian model which incorporates recent game results to
    estimate the momentum the team currently has (also could possibly
    marginalize over the momentum lost by player injuries?)

    Arguments:
        data: The features generated by featuregen.py to train from
    """
    return NotImplementedError()


# The collection of models trainable on. The 'model' command line argument
# must match one of the keys
_MODELS = {'naive_non_stat': train_naive_non_stat_bayes,
           'naive_stat': train_naive_stat_bayes
           }


def parse_args():
    """To get the necessary arguments from the command line
    """
    parser = argparse.ArgumentParser(
        description='Training a model and getting an estimated level of '
                    'accuracy using Leave-One-Out cross validation.')
    parser.add_argument('data', type=str,
                        help='The feature, labels datasets to train on. '
                             'Should have been generated by feature_gen.py')
    parser.add_argument('model', type=str,
                        help='The type of model to train. Can select from: ' +
                             ', '.join(list(_MODELS.keys())))
    parser.add_argument('-v', '--verbose', action='store_true',
                        help='Whether to output more training information '
                             'execution and training.')
    return parser.parse_args()


def main():
    """When called from the command line
    """
    args = parse_args()

    X = np.array((0, 0))
    y = np.array((0, 0))
    try:
        with open(args.data, 'r') as f:
            X, y = json.load(f)
        # convert back to numpy since json can't serialize numpy arrays
        X = np.array(X, dtype=None)
        y = np.array(y, dtype=None)

    except:
        print('COULDN\'T OPEN', args.data)
        exit(1)

    if args.model not in _MODELS:
        print('INVALID MODEL')
        exit(1)

    # run the training routine
    _MODELS[args.model](X, y, verbose=args.verbose)


if __name__ == '__main__':
    main()
